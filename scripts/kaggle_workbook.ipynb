{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import humanfriendly\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile, ImageFont, ImageDraw\n",
    "import statistics\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior()\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "from CameraTraps.ct_utils import truncate_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.2.0\n",
      "Is GPU available? tf.test.is_gpu_available: True\n"
     ]
    }
   ],
   "source": [
    "# ignoring all \"PIL cannot read EXIF metainfo for the images\" warnings\n",
    "warnings.filterwarnings('ignore', '(Possibly )?corrupt EXIF data', UserWarning)\n",
    "\n",
    "# Metadata Warning, tag 256 had too many entries: 42, expected 1\n",
    "warnings.filterwarnings('ignore', 'Metadata warning', UserWarning)\n",
    "\n",
    "# Numpy FutureWarnings from tensorflow import\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('Is GPU available? tf.test.is_gpu_available:', tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"../iwildcam2022-fgvc9_data/\"\n",
    "train_dir = root_dir + \"train/train/\"\n",
    "test_dir = root_dir + \"test/test/\"\n",
    "metadata_dir = root_dir + \"metadata/metadata/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images:  60029\n"
     ]
    }
   ],
   "source": [
    "train_data = os.listdir(train_dir)\n",
    "test_data = os.listdir(test_dir)\n",
    "meta_data_file = os.listdir(metadata_dir)\n",
    "print(\"number of images: \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = {}\n",
    "for file in meta_data_file:\n",
    "    if file.endswith('json'):\n",
    "        with open(metadata_dir + file) as f:\n",
    "            meta_data[file] = json.load(f)\n",
    "    if file.endswith('csv'):\n",
    "            meta_data[file] = pd.read_csv(metadata_dir + file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/microsoft/CameraTraps/blob/master/visualization/visualization_utils.py\n",
    "\n",
    "def open_image(input_file):\n",
    "    \"\"\"\n",
    "    Opens an image in binary format using PIL.Image and convert to RGB mode. This operation is lazy; image will\n",
    "    not be actually loaded until the first operation that needs to load it (for example, resizing), so file opening\n",
    "    errors can show up later.\n",
    "    Args:\n",
    "        input_file: an image in binary format read from the POST request's body or\n",
    "            path to an image file (anything that PIL can open)\n",
    "    Returns:\n",
    "        an PIL image object in RGB mode\n",
    "    \"\"\"\n",
    "\n",
    "    image = Image.open(input_file)\n",
    "    if image.mode not in ('RGBA', 'RGB', 'L'):\n",
    "        raise AttributeError('Input image {} uses unsupported mode {}'.format(input_file, image.mode))\n",
    "    if image.mode == 'RGBA' or image.mode == 'L':\n",
    "        # PIL.Image.convert() returns a converted copy of this image\n",
    "        image = image.convert(mode='RGB')\n",
    "    return image\n",
    "\n",
    "def load_image(input_file):\n",
    "    \"\"\"\n",
    "    Loads the image at input_file as a PIL Image into memory; Image.open() used in open_image() is lazy and\n",
    "    errors will occur downstream if not explicitly loaded\n",
    "    Args:\n",
    "        input_file: an image in binary format read from the POST request's body or\n",
    "            path to an image file (anything that PIL can open)\n",
    "    Returns:\n",
    "        an PIL image object in RGB mode\n",
    "    \"\"\"\n",
    "    image = open_image(input_file)\n",
    "    image.load()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFDetector:\n",
    "    \"\"\"\n",
    "    A detector model loaded at the time of initialization. It is intended to be used with\n",
    "    the MegaDetector (TF). The inference batch size is set to 1; code needs to be modified\n",
    "    to support larger batch sizes, including resizing appropriately.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of decimal places to round to for confidence and bbox coordinates\n",
    "    CONF_DIGITS = 3\n",
    "    COORD_DIGITS = 4\n",
    "\n",
    "    # MegaDetector was trained with batch size of 1, and the resizing function is a part\n",
    "    # of the inference graph\n",
    "    BATCH_SIZE = 1\n",
    "\n",
    "    # An enumeration of failure reasons\n",
    "    FAILURE_TF_INFER = 'Failure TF inference'\n",
    "    FAILURE_IMAGE_OPEN = 'Failure image access'\n",
    "\n",
    "    DEFAULT_RENDERING_CONFIDENCE_THRESHOLD = 0.85  # to render bounding boxes\n",
    "    DEFAULT_OUTPUT_CONFIDENCE_THRESHOLD = 0.1  # to include in the output json file\n",
    "\n",
    "    DEFAULT_DETECTOR_LABEL_MAP = {\n",
    "        '1': 'animal',\n",
    "        '2': 'person',\n",
    "        '4': 'vehicle'  # will be available in megadetector v4\n",
    "    }\n",
    "\n",
    "    NUM_DETECTOR_CATEGORIES = 4  # animal, person, group, vehicle - for color assignment\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        \"\"\"Loads the model at model_path and start a tf.Session with this graph. The necessary\n",
    "        input and output tensor handles are obtained also.\"\"\"\n",
    "        detection_graph = TFDetector.__load_model(model_path)\n",
    "        self.tf_session = tf.Session(graph=detection_graph)\n",
    "\n",
    "        self.image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        self.box_tensor = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        self.score_tensor = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        self.class_tensor = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "\n",
    "    @staticmethod\n",
    "    def round_and_make_float(d, precision=4):\n",
    "        return truncate_float(float(d), precision=precision)\n",
    "\n",
    "    @staticmethod\n",
    "    def __convert_coords(np_array):\n",
    "        \"\"\" Two effects: convert the numpy floats to Python floats, and also change the coordinates from\n",
    "        [y1, x1, y2, x2] to [x1, y1, width_box, height_box] (in relative coordinates still).\n",
    "        Args:\n",
    "            np_array: array of predicted bounding box coordinates from the TF detector\n",
    "        Returns: array of predicted bounding box coordinates as Python floats and in [x1, y1, width_box, height_box]\n",
    "        \"\"\"\n",
    "        # change from [y1, x1, y2, x2] to [x1, y1, width_box, height_box]\n",
    "        width_box = np_array[3] - np_array[1]\n",
    "        height_box = np_array[2] - np_array[0]\n",
    "\n",
    "        new = [np_array[1], np_array[0], width_box, height_box]  # cannot be a numpy array; needs to be a list\n",
    "\n",
    "        # convert numpy floats to Python floats\n",
    "        for i, d in enumerate(new):\n",
    "            new[i] = TFDetector.round_and_make_float(d, precision=TFDetector.COORD_DIGITS)\n",
    "        return new\n",
    "\n",
    "    @staticmethod\n",
    "    def __load_model(model_path):\n",
    "        \"\"\"Loads a detection model (i.e., create a graph) from a .pb file.\n",
    "        Args:\n",
    "            model_path: .pb file of the model.\n",
    "        Returns: the loaded graph.\n",
    "        \"\"\"\n",
    "        print('TFDetector: Loading graph...')\n",
    "        detection_graph = tf.Graph()\n",
    "        with detection_graph.as_default():\n",
    "            od_graph_def = tf.GraphDef()\n",
    "            with tf.gfile.GFile(model_path, 'rb') as fid:\n",
    "                serialized_graph = fid.read()\n",
    "                od_graph_def.ParseFromString(serialized_graph)\n",
    "                tf.import_graph_def(od_graph_def, name='')\n",
    "        print('TFDetector: Detection graph loaded.')\n",
    "\n",
    "        return detection_graph\n",
    "\n",
    "    def _generate_detections_one_image(self, image):\n",
    "        np_im = np.asarray(image, np.uint8)\n",
    "        im_w_batch_dim = np.expand_dims(np_im, axis=0)\n",
    "\n",
    "        # need to change the above line to the following if supporting a batch size > 1 and resizing to the same size\n",
    "        # np_images = [np.asarray(image, np.uint8) for image in images]\n",
    "        # images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)\n",
    "\n",
    "        # performs inference\n",
    "        (box_tensor_out, score_tensor_out, class_tensor_out) = self.tf_session.run(\n",
    "            [self.box_tensor, self.score_tensor, self.class_tensor],\n",
    "            feed_dict={self.image_tensor: im_w_batch_dim})\n",
    "\n",
    "        return box_tensor_out, score_tensor_out, class_tensor_out\n",
    "\n",
    "    def generate_detections_one_image(self, image, image_id,\n",
    "                                      detection_threshold=DEFAULT_OUTPUT_CONFIDENCE_THRESHOLD):\n",
    "        \"\"\"Apply the detector to an image.\n",
    "        Args:\n",
    "            image: the PIL Image object\n",
    "            image_id: a path to identify the image; will be in the `file` field of the output object\n",
    "            detection_threshold: confidence above which to include the detection proposal\n",
    "        Returns:\n",
    "        A dict with the following fields, see https://github.com/microsoft/CameraTraps/tree/siyu/inference_refactor/api/batch_processing#batch-processing-api-output-format\n",
    "            - image_id (always present)\n",
    "            - max_detection_conf\n",
    "            - detections, which is a list of detection objects containing `category`, `conf` and `bbox`\n",
    "            - failure\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            'file': image_id\n",
    "        }\n",
    "        try:\n",
    "            b_box, b_score, b_class = self._generate_detections_one_image(image)\n",
    "\n",
    "            # our batch size is 1; need to loop the batch dim if supporting batch size > 1\n",
    "            boxes, scores, classes = b_box[0], b_score[0], b_class[0]\n",
    "\n",
    "            detections_cur_image = []  # will be empty for an image with no confident detections\n",
    "            max_detection_conf = 0.0\n",
    "            for b, s, c in zip(boxes, scores, classes):\n",
    "                if s > detection_threshold:\n",
    "                    detection_entry = {\n",
    "                        'category': str(int(c)),  # use string type for the numerical class label, not int\n",
    "                        'conf': truncate_float(float(s),  # cast to float for json serialization\n",
    "                                               precision=TFDetector.CONF_DIGITS),\n",
    "                        'bbox': TFDetector.__convert_coords(b)\n",
    "                    }\n",
    "                    detections_cur_image.append(detection_entry)\n",
    "                    if s > max_detection_conf:\n",
    "                        max_detection_conf = s\n",
    "\n",
    "            result['max_detection_conf'] = truncate_float(float(max_detection_conf),\n",
    "                                                          precision=TFDetector.CONF_DIGITS)\n",
    "            result['detections'] = detections_cur_image\n",
    "\n",
    "        except Exception as e:\n",
    "            result['failure'] = TFDetector.FAILURE_TF_INFER\n",
    "            print('TFDetector: image {} failed during inference: {}'.format(image_id, str(e)))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/microsoft/CameraTraps/blob/master/detection/run_tf_detector.py\n",
    "\n",
    "def load_and_run_detector(model_file, image_file_names, output_dir,\n",
    "                          render_confidence_threshold=TFDetector.DEFAULT_RENDERING_CONFIDENCE_THRESHOLD):\n",
    "    if len(image_file_names) == 0:\n",
    "        print('Warning: no files available')\n",
    "        return\n",
    "\n",
    "    # load and run detector on target images, and visualize the results\n",
    "    start_time = time.time()\n",
    "    tf_detector = TFDetector(model_file)\n",
    "    elapsed = time.time() - start_time\n",
    "    print('Loaded model in {}'.format(humanfriendly.format_timespan(elapsed)))\n",
    "\n",
    "    detection_results = []\n",
    "    time_load = []\n",
    "    time_infer = []\n",
    "    detection_categories = []\n",
    "\n",
    "    # since we'll be writing a bunch of files to the same folder, rename\n",
    "    # as necessary to avoid collisions\n",
    "    output_file_names = {}\n",
    "\n",
    "    for im_file in tqdm(image_file_names):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            image = load_image(im_file)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            time_load.append(elapsed)\n",
    "#             print(time_load)\n",
    "        except Exception as e:\n",
    "#             print('Image {} cannot be loaded. Exception: {}'.format(im_file, e))\n",
    "            result = {\n",
    "                'file': im_file,\n",
    "                'failure': TFDetector.FAILURE_IMAGE_OPEN\n",
    "            }\n",
    "            detection_results.append(result)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            result = tf_detector.generate_detections_one_image(image, im_file)\n",
    "            \n",
    "            #print(\"Detection result is:\", result)\n",
    "            \n",
    "            detection_results.append(result)\n",
    "            \n",
    "            if result[\"detections\"] == []:\n",
    "                detection_categories.append(0)\n",
    "            else:\n",
    "                    \n",
    "                detection_categories.append(result[\"detections\"][0][\"category\"])\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            time_infer.append(elapsed)\n",
    "        except Exception as e:\n",
    "            print('An error occurred while running the detector on image {}. Exception: {}'.format(im_file, e))\n",
    "            # the error code and message is written by generate_detections_one_image,\n",
    "            # which is wrapped in a big try catch\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # image is modified in place\n",
    "            render_detection_bounding_boxes(result['detections'], image,\n",
    "                                                      label_map=TFDetector.DEFAULT_DETECTOR_LABEL_MAP,\n",
    "                                                      confidence_threshold=render_confidence_threshold)\n",
    "            fn = os.path.basename(im_file).lower()\n",
    "            name, ext = os.path.splitext(fn)\n",
    "            fn = '{}{}{}'.format(name, ImagePathUtils.DETECTION_FILENAME_INSERT, '.jpg')  # save all as JPG\n",
    "            if fn in output_file_names:\n",
    "                n_collisions = output_file_names[fn]  # if there were a collision, the count is at least 1\n",
    "                fn = str(n_collisions) + '_' + fn\n",
    "                output_file_names[fn] = n_collisions + 1\n",
    "            else:\n",
    "                output_file_names[fn] = 0\n",
    "\n",
    "            output_full_path = os.path.join(output_dir, fn)\n",
    "            image.save(output_full_path)\n",
    "\n",
    "        except Exception as e:\n",
    "#             print('Visualizing results on the image {} failed. Exception: {}'.format(im_file, e))\n",
    "            continue\n",
    "\n",
    "    ave_time_load = statistics.mean(time_load)\n",
    "    ave_time_infer = statistics.mean(time_infer)\n",
    "    if len(time_load) > 1 and len(time_infer) > 1:\n",
    "        std_dev_time_load = humanfriendly.format_timespan(statistics.stdev(time_load))\n",
    "        std_dev_time_infer = humanfriendly.format_timespan(statistics.stdev(time_infer))\n",
    "    else:\n",
    "        std_dev_time_load = 'not available'\n",
    "        std_dev_time_infer = 'not available'\n",
    "    print('On average, for each image,')\n",
    "    print('- loading took {}, std dev is {}'.format(humanfriendly.format_timespan(ave_time_load),\n",
    "                                                    std_dev_time_load))\n",
    "    print('- inference took {}, std dev is {}'.format(humanfriendly.format_timespan(ave_time_infer),\n",
    "                                                      std_dev_time_infer))\n",
    "    \n",
    "    return detection_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_files = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    img_files.append(test_dir + test_data[i])\n",
    "# img_file = [train_dir + train_data[0]]\n",
    "model_file = \"../model/md_v4.1.0.pb\"\n",
    "\n",
    "output_dir = \"./result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFDetector: Loading graph...\n",
      "TFDetector: Detection graph loaded.\n",
      "Loaded model in 3.58 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 21683/60029 [2:04:33<3:35:37,  2.96it/s]"
     ]
    }
   ],
   "source": [
    "detect_result = load_and_run_detector(model_file, img_files, output_dir, render_confidence_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1ed8b2f6104c32086caac65b6e469de57d2195192314ff8df1e75c600b24b89"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
